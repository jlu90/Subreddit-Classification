{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 - Life Pro Tips vs. Unethical Life Pro Tips Classification\n",
    "**Author:** Jocelyn Lutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "A \"life pro tip\" is defined as a concise and specific tip that can improve people's lives. The popular website, Reddit has many forums dedicated to helping people improve their lives. Two of the more popular subreddits to gain these tidbits of advice are [r/LifeProTips](https://www.reddit.com/r/LifeProTips/) and [r/UnethicalLifeProTips](https://www.reddit.com/r/UnethicalLifeProTips/). Both provide advice, but due to the nature of the content, their mission statements are very different.\n",
    "\n",
    "[r/LifeProTips](https://www.reddit.com/r/LifeProTips/) has the mission to provide \"tips that improve your life in one way or another\". In order to ensure that the subreddit does provide good advice, the moderators of the site specifically ban any tips deemed to be common sense, common courtesies or unethical. Any posts suggesting behaviors that are illegal in the United States are also banned. \n",
    "\n",
    "[r/UnethicalLifeProTips](https://www.reddit.com/r/UnethicalLifeProTips/) is the complete of opposite of [r/LifeProTips](https://www.reddit.com/r/LifeProTips/). They allow users to post tips that will likely improve lives but might be harmful to others and/or condone behaviors bordering on illegal. Although the moderators suggest the advice is just for fun and should not be followed, several people come to the site to find and request unethical advice. \n",
    "\n",
    "Given the constrasting missions of the two subreddits, it is imperative that no Unethical Life Pro Tips ever be posted to [r/LifeProTips](https://www.reddit.com/r/LifeProTips/). However, in a recent update to the popular website Reddit, a backend engineer accidentally programmed all of the posts that were submitted to the 'Unethical Life Pro Tips' subreddit to be posted to the 'Life Pro Tips' subreddit. ***As the data scientist on the team, I have been assigned the task of building a classificiation model that can accurately classify posts from each subreddit so that r/Life Pro Tips can be free of unethical advice and [r/UnethicalLifeProTips](https://www.reddit.com/r/UnethicalLifeProTips/) can continue to offer new, unethical content.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data was collected from r/LifeProTips and r/UnethicalLifeProTips on July 18, 2020 using the [Pushift Reddit API](https://github.com/pushshift/api). In total, 2677 posts from r/LifeProTips were collected, corresponding to 15 days worth of data, and 3961 posts from r/UnethicalLifeProTips were collected, corresponding to approximately 40 weeks of data. Posts were aggreggated into a single data frame and data was cleaned according to standard protocol (handling null values, confirming data types, and searching for any abnormal values). Because text data can contain many symbols and phrases that do not add value to a sentence, any symbols or tags (e.g. '[request]') were removed to the best of my abilities. Additionally, in order to make the task more challenging, the 'LPT' and 'ULPT' tags were also removed from posts. Basic features were engineered to allow for examination of full text in the post, post length in characters, post length in words, and sentiment polarity (-1 = Negative, 1 = Positive).\n",
    "\n",
    "Once the data had been clean, text preprocessing was conducted to expand contractions in the text, and posts were tokenized and  words were reduced to their word stems using a Porter stemmer. In order to be compatible with the text vectorizers, the stemmed words were joined as a string. At this point, the data frame was considered ready for exploratory data analysis and modeling. The data dictionary can be found below. \n",
    "\n",
    "During EDA, distributions for all numeric features were explored, the most frequent word counts were generated per subreddit. Sentiment analysis with TextBlob was also conducted to determine if polarity differed by subreddit. \n",
    "\n",
    "Following EDA, the data was prepared for modeling. Prior to beginning the modeling process, preliminary analyses were conducted to determine the best vectorizer for the data. These analyses revealed that a Tfidf Vectorizer performed better than a Count Vectorizer in Logisitic Regression models, so only Tfidf was used for these models. A 70/30 train test split was conducted, and a column transformer with a Tfidf Vectorizer was created to vectorizer the stemmed text, while also maintaining the original form of any numeric features. \n",
    "\n",
    "During the modeling process, for each model (except for the Baseline/Null Model), a pipeline was created. Pipelines contained the column transformer (Tfidf Vectorizer) and a classification model. GridSearchCV was used to tune hyperparameters, and model performance was evaluated using the accuracy metric. The models built were: \n",
    "1. Baseline/Null Model\n",
    "2. Logistic Regression\n",
    "3. Logistic Regression with L2 Regularization\n",
    "4. Multinomial Bayes\n",
    "5. K-Nearest Neighbors (KNN)\n",
    "6. Decision Tree\n",
    "7. Bagging Classifier\n",
    "8. Random Forest\n",
    "9. Voting Classifier\n",
    "\n",
    "In the following steps, my work flow will be described, and I will select and evaluate a model to classify by posts from the r/LifeProTips and r/UnethicalLifeProTips subreddits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Data Dictionary](#Data-Dictionary)\n",
    "2. [Imports](#Imports)\n",
    "3. [Read-In Data](#Read-In-Data)\n",
    "4. [Data Cleaning](#Data-Cleaning)\n",
    "5. [Basic Feature Engineering](#Basic-Feature-Engineering)\n",
    "6. [Text Preprocessing](#Text-Preprocessing)\n",
    "7. [Model Preparation](#Model-Preparation)\n",
    "8. [Modeling](#Modeling)\n",
    "    * [Baseline/Null Model](#Model-1:-Null_Model)\n",
    "    * [Logistic Regression with No Regularization](#Model-2a:-Logistic-Regression-with-No-Regularization)\n",
    "    * [Logistic Regression with Regularization](#Model-2b:-Logistic-Regression-with-Regularization)\n",
    "    * [Multinomial Naive Bayes](#Model-3:-Multinomial-Naive-Bayes)\n",
    "    * [KNN](#Model-4:-KNN)\n",
    "    * [Decision Tree](#Model-5:-Decision-Tree)\n",
    "    * [Bagging Classifier](#Model-6:-Bagging-Classifier)\n",
    "    * [Random Forest](#Model-7:-Random-Forest)\n",
    "    * [Voting Classifier](#Model-8:-Voting-Classifier)\n",
    "9. [Model Selection](#Model-Selection)\n",
    "10. [Model Evaluation](#Model-Evaluation)\n",
    "11. [Model Interpretation](#Model-Interpretation)\n",
    "12. [Conclusions](#Conclusions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dictionary\n",
    "\n",
    "The clean, preprocessed data used for EDA and modeling can be found [here](https://git.generalassemb.ly/jlu90/project_3/blob/master/data/subreddits_preprocessed.csv).\n",
    "\n",
    "|Column| Data Type| Description|\n",
    "|---|---|---|\n",
    "subreddit| String| Source of data\n",
    "suthor|String| Author of post\n",
    "num_comments|Integer|Number of comments a post received\n",
    "score|Integer|Score of post\n",
    "timestamp|Datetime|Date of post\n",
    "original_text|String|Text from original post\n",
    "post_length_char|Integer|Character length of post\n",
    "post_length_words|Integer|Word count of post\n",
    "is_unethical|Integer|Binary variable denoting if a post came from Unethical Life Pro Tips\n",
    "stemmer_text|String|Post with stemmed text\n",
    "polarity|Float|Positive or negative sentiment of post ranging from 1 (positive) to -1 (negative)\n",
    "sentiment_cat|String|Category of sentiment based on polarity score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read-In Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = pd.read_csv('../data/subreddits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>is_self</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>LPT: Answers to why</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>LifeProTips</td>\n",
       "      <td>1595040153</td>\n",
       "      <td>AlienAgency</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-07-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>¿Quieres obtener juegos y premios gratis en tu...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>LifeProTips</td>\n",
       "      <td>1595040616</td>\n",
       "      <td>GarbageMiserable0x0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-07-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title   selftext  \\\n",
       "0           0                                LPT: Answers to why  [removed]   \n",
       "1           1  ¿Quieres obtener juegos y premios gratis en tu...  [removed]   \n",
       "\n",
       "     subreddit  created_utc               author  num_comments  score  \\\n",
       "0  LifeProTips   1595040153          AlienAgency             2      1   \n",
       "1  LifeProTips   1595040616  GarbageMiserable0x0             2      1   \n",
       "\n",
       "   is_self   timestamp  \n",
       "0     True  2020-07-17  \n",
       "1     True  2020-07-17  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddits.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Unnecessary Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop 'Unnamed: 0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits.drop(columns = 'Unnamed: 0', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop 'created_utc' column and 'is_self' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits.drop(columns = ['created_utc', 'is_self'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Title Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove 'UPLT Requests' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, title in enumerate(subreddits['title']):\n",
    "    if 'ulpt request' in title.lower():\n",
    "        subreddits.drop(index, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove 'LPT' and ULPT' from title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for title in subreddits['title']:\n",
    "    if 'LPT'.lower() in title.lower() or 'ULPT'.lower() in title.lower():\n",
    "        subreddits['title'] = subreddits['title'].str.replace('ULPT', '')\n",
    "        subreddits['title'] = subreddits['title'].str.replace('LPT', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove '&amp;' symbol identified during EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for post in subreddits['title']:\n",
    "    if '&amp;' in post.lower():\n",
    "        subreddits['title'] = subreddits['title'].str.replace('&amp;', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean 'SelfText' Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill Null Values with an Empty String and Replace any '[removed]' tags with an empty string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits['selftext'].fillna('', inplace = True)\n",
    "subreddits['selftext'] = subreddits['selftext'].str.replace('\\[removed\\]', '')\n",
    "subreddits['selftext'] = subreddits['selftext'].str.replace('\\[\\]', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine the percentage of rows that are missing 'selftext' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6529291553133515"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subreddits[subreddits['selftext']=='']['selftext'])/len(subreddits['selftext'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 65% of rows are missing 'selftext'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UnethicalLifeProTips    0.568336\n",
       "LifeProTips             0.431664\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddits[subreddits['selftext']=='']['subreddit'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there is a greater percentage of 'selftext' missing from r/UnethicalLifeProTips, percent missing is not too different between the two subreddits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean symbols discovered during EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for post in subreddits['selftext']:\n",
    "    subreddits['selftext'] = subreddits['selftext'].str.replace('x200B', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for post in subreddits['selftext']:\n",
    "    if '&amp;' in post.lower():\n",
    "        subreddits['selftext'] = subreddits['selftext'].str.replace('&amp;', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean 'Num_Comments' Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddits['num_comments'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5872.000000\n",
       "mean       16.246935\n",
       "std       125.518164\n",
       "min         0.000000\n",
       "25%         1.000000\n",
       "50%         2.000000\n",
       "75%         6.000000\n",
       "max      5091.000000\n",
       "Name: num_comments, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddits['num_comments'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within this dataset, 75% of posts have 6 or fewer comments. Only 146 posts have 100 or more comments, and only 8 posts have 1000 or more comments. Because the posts with more comments could be exclusively featured in one subreddit or the other, I will not drop any rows, even if they seem like outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean 'Score' Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddits['score'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     5872.000000\n",
       "mean        32.774183\n",
       "std        606.188675\n",
       "min          0.000000\n",
       "25%          1.000000\n",
       "50%          1.000000\n",
       "75%          1.000000\n",
       "max      28606.000000\n",
       "Name: score, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddits['score'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, 75% of all posts in this data set have a score of 1 or below. Only 81 posts have a score of 100 or more, and only 25 posts have a score of 1000 or higher. Because the high scores could be characteristic of one subreddit, I will not drop any rows based on score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean 'Timestamp' Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddits['timestamp'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count           5872\n",
       "unique           132\n",
       "top       2020-07-04\n",
       "freq             219\n",
       "Name: timestamp, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddits['timestamp'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The timestamp data looks okay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean 'Author' Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA revealed that one user 'h3xadecimal2' has posted the same spam post to both subreddits. These posts will not contribute meaningful data to our model, so the posts by this user will be dropped from the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = subreddits[subreddits['author']!='h3xadecimal2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were also three posts by authors of multiple posts that had the same text, so one of the posts will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits.drop(117, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits.drop(569, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits.drop(2145, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon looking at the data, there are some features that I know that I would like to create to without exploring the data. I will create these features here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineer a Total Text Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the subreddits only have titles and no self text. However, these posts still provide a life pro tip. In order to get rid of the null values in the self text column while still preserving the entire post, I will combine the text columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits['total_text'] = subreddits['title'] + subreddits['selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits.drop(columns = ['title', 'selftext'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineer a Post Length (Characters) Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits['post_length_char'] = subreddits['total_text'].map(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineer a Post Length (Word Count) Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits['post_length_words'] = subreddits['total_text'].map(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Target to Binary Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits['is_unethical'] = subreddits['subreddit'].map({'LifeProTips': 0, 'UnethicalLifeProTips': 1 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.544168\n",
       "0    0.455832\n",
       "Name: is_unethical, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddits['is_unethical'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits.to_csv('../data/subreddits_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The original work flow was conducted in multiple notebooks. Therefore, where ever a new notebook would start, the data will be re-read in to ensure proper indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = pd.read_csv('../data/subreddits_clean.csv')\n",
    "\n",
    "subreddits.drop(columns = 'Unnamed: 0', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\n",
    "    \"aren't\": 'are not',\n",
    "    \"can't\": 'cannot',\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesnt\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\", \n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i am\": \"i'm\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"she'd\":\"she would\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"shouldn't\":\"should not\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they had\",\n",
    "    \"they'll\":\"they will\",\n",
    "    \"they're\": \"they are\", \n",
    "    \"they've\": \"they have\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\", \n",
    "    \"weren't\": \"were not\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"who'd\": \"who would\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_indices = []\n",
    "\n",
    "for index, post in enumerate(subreddits['total_text']):\n",
    "    for word in post.split():\n",
    "        if word.lower() in contractions.keys(): # Check if word is a contraction\n",
    "            contraction_indices.append((index, word)) # Add the index and contraction to a list\n",
    "\n",
    "for entry in contraction_indices:\n",
    "    subreddits.loc[entry[0], 'total_text'] = subreddits.loc[entry[0], 'total_text'].replace(entry[1].lower(), contractions[entry[1].lower()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stem Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In order to decide between lemmatization and stemming, I created Logistic Regression models to see which seemed to perform best. Models with stemmed text had highest accuracy in those models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tokens for Stemming\n",
    "\n",
    "post_tokens = []\n",
    "\n",
    "for post in subreddits['total_text']:\n",
    "    post_tokens.append(word_tokenize(post.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem Words and Convert Back to String\n",
    "\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "post_stemmer_tokens = []\n",
    "\n",
    "for token in post_tokens:\n",
    "    running_stemmer_tokens = []\n",
    "    for word in token:\n",
    "        running_stemmer_tokens.append(p_stemmer.stem(word))\n",
    "    post_stemmer_tokens.append(' '.join(running_stemmer_tokens))\n",
    "\n",
    "subreddits['stemmer_text'] = post_stemmer_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename `total_text` column to `original_text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits.rename(columns = {'total_text':'original_text'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Sentiment Score for Posts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Text Blob](https://textblob.readthedocs.io/en/dev/) provides a sentiment analyzer. Scores range from -1 (Negative) to 1 (Positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polarity(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "subreddits['polarity'] = subreddits['original_text'].apply(get_polarity)\n",
    "\n",
    "# https://towardsdatascience.com/having-fun-with-textblob-7e9eed783d3f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentiment categories\n",
    "\n",
    "positive_mask = subreddits['polarity'] > 0\n",
    "negative_mask = subreddits['polarity'] < 0\n",
    "neutral_mask = subreddits['polarity'] == 0\n",
    "\n",
    "subreddits.loc[positive_mask, 'sentiment_cat'] = 'Positive'\n",
    "subreddits.loc[negative_mask, 'sentiment_cat'] = 'Negative'\n",
    "subreddits.loc[neutral_mask, 'sentiment_cat'] = 'Neutral'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Preprocessed Text Data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits.to_csv('../data/subreddits_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = pd.read_csv('../data/subreddits_preprocessed.csv')\n",
    "subreddits.drop(columns = 'Unnamed: 0', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Palette for Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red = '#e63946'\n",
    "cream = '#f1faee'\n",
    "light_blue = '#a8dadc'\n",
    "medium_blue = '#457b9d'\n",
    "navy = '#1d3557'\n",
    "\n",
    "custom_cmap = ListedColormap([cream, light_blue, medium_blue, navy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_labels(title, xlabel, ylabel, title_size = 16, axis_size = 13):\n",
    "    plt.title(title, fontdict = {'fontsize':title_size}, pad = 10)\n",
    "    plt.xlabel(xlabel, fontdict = {'fontsize': axis_size})\n",
    "    plt.ylabel(ylabel, fontdict = {'fontsize':13});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_v_boxplot(df, feature, fig_size = (8,6)):\n",
    "    plt.figure(figsize=fig_size)\n",
    "    sns.boxplot(df[feature], orient = 'v', color = 'silver')\n",
    "    plt.tight_layout;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hist(df, feature, num_bins=10):\n",
    "    sns.distplot(df[feature], bins = num_bins, kde = False, hist_kws=dict(edgecolor=\"k\", linewidth=2), color = navy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bar_xticks(df, x_col, y_col, hue=None, color = navy, plot_size = (10,6), lw = 1.5, x_tick_labels = None, palette = None):\n",
    "    plt.figure(figsize=plot_size)\n",
    "    ax = sns.barplot(x = x_col, y = y_col, data = df, hue = hue, color = color, linewidth = lw, edgecolor = 'black', palette = palette)\n",
    "    ax.set_xticklabels(x_tick_labels)\n",
    "    plt.tight_layout;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequent_words(df, words_df, feature, binary_num):\n",
    "    \n",
    "    # Generate Word Counts\n",
    "    feature_var = df[df[feature] == binary_num]\n",
    "    feature_word_counts = {}\n",
    "\n",
    "    for column in words_df.columns:\n",
    "        feature_word_counts[column] = feature_var[column].sum()\n",
    "\n",
    "    feature_word_counts_df = pd.DataFrame(sorted(feature_word_counts.items(), key = lambda x: x[1], reverse = True), columns = ['Word', 'Count'])\n",
    "    return feature_word_counts_df.sort_values('Count', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_frequent_words(word_count_df, subreddit, color):\n",
    "    plt.figure(figsize = (18, 6))\n",
    "    sns.barplot(word_count_df['Word'][:25], word_count_df['Count'].sort_values(ascending = False)[:25], color = color, edgecolor = 'black')\n",
    "    plt.title(f'25 Most Commonly Used Words in {subreddit}', fontdict = {'fontsize':15}, pad = 12)\n",
    "    plt.xlabel('Word', fontdict = {'fontsize':13}, labelpad = 10)\n",
    "    plt.ylabel('Count', fontdict = {'fontsize':13}, labelpad = 10)\n",
    "    plt.tight_layout;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration of Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpt_authors = subreddits[subreddits['subreddit'] == 'LifeProTips']['author']\n",
    "ulpt_authors = subreddits[subreddits['subreddit'] == 'UnethicalLifeProTips']['author']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What percentage of authors are unique?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpt_authors.nunique()/len(lpt_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ulpt_authors.nunique()/len(ulpt_authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percentage of unique authors between the two subreddits is similar, but r/UnethicalLifeProTips has 4% more unique authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do any authors appear in both subreddits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpt_authors_set = set(lpt_authors)\n",
    "ulpt_authors_set = set(ulpt_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_in_common = lpt_authors_set & ulpt_authors_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Authors in Common:', len(authors_in_common)) # Intersection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 17 authors who have posted on both subreddits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Distributions of Numerical Data Between Subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "life_pro_tips = subreddits[subreddits['subreddit'] == 'LifeProTips']\n",
    "unethical_life_pro_tips = subreddits[subreddits['subreddit'] == 'UnethicalLifeProTips']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_v_boxplot(life_pro_tips, 'num_comments')\n",
    "set_labels('Distribution of Number of Comments for Life Pro Tips Posts', 'Life Pro Tips', 'Number of Comments');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the boxplot above, the range of number of comments for Life Pro Tips posts, is highly spread out. The minimum number of comments is 0, and the maximum number of comments is 5091. The interquartile range is from 2 comments to 8 comments. Only 653 out of the 2678 posts have more than 8 comments. These 653 posts appear as outliers in the boxplot. However, because posts with more comments could be a distinguishing feature between the subreddits, all points outside of the interquartile range will be kept in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_v_boxplot(unethical_life_pro_tips, 'num_comments')\n",
    "set_labels('Distribution of Number of Comments for Unethical Life Pro Tips Posts', 'Unethical Life Pro Tips', 'Number of Comments');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the number of comments for Life Pro Tips Posts, we see that the range for number of comments for posts in Unethical Life Pro Tips is also highly spread out. The minimum number of comments is 0, and the maximum number of comments is 786. For Unethical Life Pro Tips, 75% of the posts have 4 or less comments. Only 791 out of 3194 comments have comment counts than 4, and many of these posts appear as outliers on the boxplot. However, again, because the variation in comments could be an important feature for distinguishing between the two subreddits, all posts will remain in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_v_boxplot(life_pro_tips, 'score')\n",
    "set_labels('Distribution of Scores for Life Pro Tips Posts', 'Life Pro Tips', 'Score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the distributions for number of comments, the range of scores for Life Pro Tips posts is also highly variable. The range is from 0 to 28606, with 75% of posts having a score of 1 or less. Only 432 out of 2678 posts have a score higher than 1. Likely the posts with a high score became very popular and were featured on the main Reddit page. Again, because these high scores could hold important information, they will not be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_v_boxplot(unethical_life_pro_tips, 'score')\n",
    "set_labels('Distribution of Scores for Unethical Life Pro Tips Posts', 'Unethical Life Pro Tips', 'Score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores for Unethical Life Pro Tips are highly variable. The min score is 0, and the max score is 18,861. 75% of all posts have a score of 1 or less, so many posts appear as outliers on this figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post Length (Characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_v_boxplot(life_pro_tips, 'post_length_char')\n",
    "set_labels('Distribution of Post Lengths (Characters) for Life Pro Tips Posts', 'Life Pro Tips', 'Length (Characters)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the boxplot above, there is a large range of post lengths in the dataset. The length in characters ranges from 1 character to 19164 characters, and 75% of all posts are 306.75 characters or less. Because the post with 19164 characters appears to be a guidebook, it will be dropped from the dataset. With the column dropped, the max length becomes 4856 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop column from original data set\n",
    "subreddits.drop(308, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_v_boxplot(unethical_life_pro_tips, 'post_length_char')\n",
    "set_labels('Distribution of Post Lengths (Characters) for Unethical Life Pro Tips Posts', 'Unethical Life Pro Tips', 'Length (Characters)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range of lengths for posts in Unethical Life Pro Tips is also highly variable. The minimum post length is 2 characters, and the maximum is 6193 characters. 75% of posts are 266 characters or shorter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post Length (Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_v_boxplot(life_pro_tips, 'post_length_words')\n",
    "set_labels('Distribution of Post Lengths (Words) for Life Pro Tips Posts', 'Life Pro Tips', 'Length (Words)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, there are still several posts whose lengths are considered outliers. The minimum length is 1 word, and the maximum length is 881 words. The median value is 29 words, and 75% of posts are 56 words or shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_v_boxplot(unethical_life_pro_tips, 'post_length_words')\n",
    "set_labels('Distribution of Post Lengths (Words) for Life Pro Tips Posts', 'Life Pro Tips', 'Length (Words)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range of variables for the word counts of Unethical Life Pro Tips posts is also large. The minimum value is 1, and the maximum value is 1125. The median length is 29, and 75% of posts are 48 words or fewer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Average Post Lengths Between Subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bar_xticks(df = subreddits,\n",
    "                 x_col = 'subreddit',\n",
    "                 y_col = 'post_length_char',\n",
    "                 x_tick_labels = ['Life Pro Tips', 'Unethical Life Pro Tips'], plot_size = (8,6))\n",
    "set_labels(title = 'Life Pro Tips Posts are Longer in Character Length', xlabel = 'Subreddit', ylabel = 'Post Length (Characters)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posts on Life Pro Tips are longer than the posts on Unethical Life Pro Tips. The mean character count for posts on Life Pro Tips is 274, while the mean character count for posts on Unethical Life Pro Tips is 238. It is important to note that the range of post lengths is big for Life Pro Tips, and this influences the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bar_xticks(df = subreddits,\n",
    "                 x_col = 'subreddit',\n",
    "                 y_col = 'post_length_words',\n",
    "                 x_tick_labels = ['Life Pro Tips', 'Unethical Life Pro Tips'], plot_size = (8,6))\n",
    "set_labels(title = 'Life Pro Tips Posts are Longer in Word Count', xlabel = 'Subreddit', ylabel = 'Post Length (Words)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posts on Life Pro Tips are longer than the posts on Unethical Life Pro Tips. The mean word count for posts on Life Pro Tips is 51.2 words, while the mean word count for posts on Unethical Life Pro Tips is 43.5 words. It is important to note that the range of post lengths is big for Life Pro Tips, and this influences the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 'Bag of Words' for EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Text\n",
    "og_corpus = subreddits['original_text']\n",
    "og_cvec = CountVectorizer(stop_words = 'english', ngram_range = (1, 2), min_df = 5)\n",
    "og_words = og_cvec.fit_transform(og_corpus)\n",
    "og_words_df = pd.DataFrame(og_words.toarray(), columns = og_cvec.get_feature_names())\n",
    "og_labeled_words_df = pd.concat([og_words_df, subreddits['is_unethical']], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemmed Text\n",
    "stemmed_corpus = subreddits['stemmer_text']\n",
    "stemmed_cvec = CountVectorizer(stop_words = 'english', ngram_range = (1, 2), min_df = 5)\n",
    "stemmed_words = stemmed_cvec.fit_transform(stemmed_corpus)\n",
    "stemmed_words_df = pd.DataFrame(stemmed_words.toarray(), columns = stemmed_cvec.get_feature_names())\n",
    "stemmed_labeled_words_df = pd.concat([stemmed_words_df, subreddits['is_unethical']], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Most Frequent Words By Subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_lpt_words_df = get_frequent_words(og_labeled_words_df, og_words_df, 'is_unethical', 0)\n",
    "plot_frequent_words(og_lpt_words_df, 'Life Pro Tips', light_blue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure shows the most common words in the Life Pro Tips Subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_ulpt_words_df = get_frequent_words(og_labeled_words_df, og_words_df, 'is_unethical', 1)\n",
    "plot_frequent_words(og_ulpt_words_df, 'Unethical Life Pro Tips', medium_blue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure shows the 25 most frequent worrds in the Unethical Life Pro Tips Subreddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Examine Overlap in Frequent Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_lpt_set = set(og_lpt_words_df.sort_values('Count', ascending = False)['Word'].tolist()[:25])\n",
    "og_ulpt_set = set(og_ulpt_words_df.sort_values('Count', ascending = False)['Word'].tolist()[:25])\n",
    "\n",
    "print('Words in Common:', og_lpt_set & og_ulpt_set) # Intersection\n",
    "print()\n",
    "print('Different Words:', og_lpt_set ^ og_ulpt_set) # Symmetric Difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, within the top 25 words in each subreddit, there is considerable overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemmed Text with Stop Words Removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpt_stemmed_words_df = get_frequent_words(stemmed_labeled_words_df, stemmed_words_df, 'is_unethical', 0)\n",
    "plot_frequent_words(lpt_stemmed_words_df, 'Life Pro Tips', light_blue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graph shows the 25 most frequent words in r/LifeProTips posts following stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ulpt_stemmed_words_df = get_frequent_words(stemmed_labeled_words_df, stemmed_words_df, 'is_unethical', 1)\n",
    "plot_frequent_words(ulpt_stemmed_words_df, 'Unethical Life Pro Tips', medium_blue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graph shows the 25 most frequent words in r/LifeProTips posts following stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_lpt_set = set(lpt_stemmed_words_df.sort_values('Count', ascending = False)['Word'][:10].tolist())\n",
    "stemmed_ulpt_set = set(ulpt_stemmed_words_df.sort_values('Count', ascending = False)['Word'][:10].tolist())\n",
    "\n",
    "print('Words in Common:', stemmed_lpt_set & stemmed_ulpt_set) # Intersection\n",
    "print()\n",
    "print('Different Words:', stemmed_lpt_set ^ stemmed_ulpt_set) # Symmetric Difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in the original text, the stemmed text also has considerable overlap between the two subreddits. Many of the overlapping stemmed words, however, appear to be stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Sentiment of Posts\n",
    "**Note:** Original Text from posts was used for sentiment analysis in order to not lose any emotional inflections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unethical Life Pro Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits[subreddits['is_unethical']==0]['polarity'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,6))\n",
    "create_hist(subreddits[subreddits['is_unethical']==0],'polarity', num_bins = 25)\n",
    "set_labels(title = 'Sentiment Analysis Results for Unethical Life Pro Tips', xlabel = 'Polarity Score', ylabel = 'Frequency', title_size = 16, axis_size = 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of posts in the Unethical Life Pro Tips subreddit were determined to have a neutral tone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Life Pro Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits[subreddits['is_unethical']==1]['polarity'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "create_hist(subreddits[subreddits['is_unethical']==1],'polarity', num_bins = 25)\n",
    "set_labels(title = 'Sentiment Analysis Results for Life Pro Tips', xlabel = 'Polarity Score', ylabel = 'Frequency', title_size = 16, axis_size = 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of posts in the Life Pro Tips subreddit also oseem to have a neutral sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the sentiment analysis reveals that the mean polarity is similar between the two subreddits' post. I will see if counts of positive, neutral, and negative posts differs between the subreddits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counts of Positive, Negative, and Neutral Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split the Polarity Scores into Categories and Calculate Proportions for Each Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Proportions of Categories \n",
    "\n",
    "x, y, hue = 'sentiment_cat', 'proportion', 'subreddit'\n",
    "\n",
    "proportions_df = (subreddits[x]\n",
    "           .groupby(subreddits[hue])\n",
    "           .value_counts(normalize=True)\n",
    "           .rename(y)\n",
    "           .reset_index())\n",
    "\n",
    "# https://github.com/mwaskom/seaborn/issues/1027"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot Bar Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "ax = sns.barplot(x=x, y=y, hue=hue, data=proportions_df, edgecolor = 'black', linewidth = 0.7, palette = [light_blue, medium_blue])\n",
    "set_labels('Proportions of Sentiments in Posts from Life Pro Tips and Unethical Life Pro Tips Subreddits', 'Sentiment', 'Proportion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the graph above and table below, it appears that posts in the Life Pro Tips subreddit might trend to be more positive and less negative than posts in the Unethical Life Pro Tips subreddit, but without error bars, it is hard to tell if this difference is significant.\n",
    "\n",
    "\n",
    "**Proportion of Posts by Sentiment and Subreddit** \n",
    "\n",
    "|Sentiment Category| Life Pro Tips| Unethical Life Pro Tips|\n",
    "|---|---|---|\n",
    "|Positive|52.1%|50.5%|\n",
    "|Neutral|25.3%|24.1%|\n",
    "|Negative|22.5%|25.3%|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After EDA, I learned that the distributions for post lengths (words and characters) are highly spread out. Although it appears that the posts in Life Pro Tips are longer than those in Unethical Life Pro Tips, the high variability in post lengths must be considered. Additionally, sentiment analysis of the posts revealed that, for both subreddits, the vast majority of posts are neutral or positive. However, looking at counts of each sentiment category revealed that posts in Life Pro Tips might be less negative than posts in Unethical Life Pro Tips.\n",
    "\n",
    "Additionally, when attempting to correctly classify the posts, EDA informed me that the amount of overlap in words between the two subreddits might prove to be challenging. In the top 500 words per subreddit, only 296 are unique. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparation\n",
    "**NOTE**: In a preliminary set of models (not shown), I determined that stemmed text and the Tfidf Vectorizer would be a good choice for my data. Therefore, I will conduct a train test split on the stemmed text and set up a Column Transformer to only vectorize my text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['num_comments', 'score', 'post_length_char', 'post_length_words', 'polarity', 'stemmer_text']\n",
    "X = subreddits[features]\n",
    "y = subreddits['is_unethical']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = RANDOM_STATE, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data for Multinomial Bayes - Only Vectorized Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bayes = subreddits[['stemmer_text']]\n",
    "y_bayes = subreddits['is_unethical']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bayes_train, X_bayes_test, y_bayes_train, y_bayes_test = train_test_split(X_bayes, y_bayes, test_size = 0.3, random_state = RANDOM_STATE, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Stop Words Hyperparameter Options for Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_stopwords = stopwords.words('english') + stopwords.words('spanish')\n",
    "custom_stop_words = nltk_stopwords + ['someon', 'll', 'like', 'know', 'ulpt', 'lpt', 'need', 'use', 'make', 'wa', 'way', 'peopl', 'ask', 'say', 'time', 'thi', 'want', 'work', 'just', 'start', 'ha', 'tri', 'becaus', 'onli', 'friend']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Column Transformer to Only Apply Vectorizer to Text Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all models except for Multinomial Naive Bayes, numeric features were included as features. Therefore, in order to vectorizer the text feature without distorting the numeric features, a column transformer was used with the TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = ColumnTransformer([\n",
    "    ('tfidf', TfidfVectorizer(), 'stemmer_text'),], \n",
    "    remainder='passthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each model, a pipeline was built that contained the TfidfVectorizer and a classification model. To the best of my abilities, hyperparameters were tuned using GridSearchCV and the tuned model was fit to the training data. Model performance was evaluated using accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_accuracy_scores(model, xtrain, ytrain, xtest, ytest):\n",
    "    print(f'The cross validation accuracy score is {round(cross_val_score(model, xtrain, ytrain).mean(),4)}.')\n",
    "    print(f'The training accuracy score is {round(model.score(xtrain, ytrain),4)}.')\n",
    "    print(f'The testing accuracy score is {round(model.score(xtest, ytest),4)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_accuracy_scores_gs(model, xtrain, ytrain, xtest, ytest):\n",
    "    print(f'The training accuracy score is {round(model.score(xtrain, ytrain),4)}.')\n",
    "    print(f'The testing accuracy score is {round(model.score(xtest, ytest),4)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_cross_val_gs(model):\n",
    "    print(f'The cross_val score is {round(model.best_score_, 4)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_sensitivity(actual_values, predicted_values):\n",
    "    tn, fp, fn, tp = confusion_matrix(actual_values, predicted_values).ravel()\n",
    "    print(f'The training sensitivity score is {round(tp/(tp+fn), 4)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_testing_sensitivity(actual_values, predicted_values):\n",
    "    tn, fp, fn, tp = confusion_matrix(actual_values, predicted_values).ravel()\n",
    "    print(f'The testing sensitivity score is {round(tp/(tp+fn), 4)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Null Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null = DummyClassifier(strategy = 'stratified', random_state = RANDOM_STATE) # Will respect training class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Evaluate Metric (Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_accuracy_scores(null, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null model is making predicitions in order to preserve the class distributions of the training set. In order to perform better than the null model, any models that are built must perform better than 50% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2a: Logistic Regression with No Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_pipe = Pipeline([\n",
    "    ('tfidf', tfidf_transformer),\n",
    "    ('logreg', LogisticRegression(penalty = 'none', solver = 'newton-cg', max_iter = 600))\n",
    "])\n",
    "\n",
    "# The model would not converge for the other solvers. Newton-cg can be used to fit larger datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search Over Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_pipe_params = {\n",
    "    'tfidf__tfidf__stop_words': [nltk_stopwords],\n",
    "    'tfidf__tfidf__ngram_range': [(1,1)],\n",
    "    'tfidf__tfidf__min_df': [6],\n",
    "    'tfidf__tfidf__max_df': [0.25],\n",
    "    'tfidf__tfidf__max_features': [750]\n",
    "}\n",
    "\n",
    "# Only best hyperparameters shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_logreg_pipe = GridSearchCV(logreg_pipe, \n",
    "                              param_grid = logreg_pipe_params, \n",
    "                              cv = 5, \n",
    "                              n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_logreg_pipe.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Metric (Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_cross_val_gs(gs_logreg_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_accuracy_scores_gs(model = gs_logreg_pipe, xtrain = X_train, xtest = X_test, ytrain = y_train, ytest = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this model performs better than the baseline model (test accuracy = 72.4%), it suffers from slight bias and high variance. Even though I limited features created by the vectorizer to 750, it is likely that this number of features is still too many for this model. If time allows, manual pruning of features based on coefficients could help to improve reduce the high variance in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2b: Logistic Regression with Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_reg_pipe = Pipeline([\n",
    "    ('tfidf', tfidf_transformer),\n",
    "    ('ss', StandardScaler(with_mean = False)), # Also tried robust scaler\n",
    "    ('logreg', LogisticRegression(solver = 'liblinear'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search Over Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_reg_pipe_params = {\n",
    "    'tfidf__tfidf__stop_words': [nltk_stopwords],\n",
    "    'tfidf__tfidf__ngram_range': [(1,2)],\n",
    "    'tfidf__tfidf__max_df': [0.25],\n",
    "    'tfidf__tfidf__min_df': [12],\n",
    "    'logreg__penalty': ['l2'],\n",
    "    'logreg__C': [0.0001]\n",
    "}\n",
    "# Only best params to minimize overfitting remain in grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_logreg_reg_pipe = GridSearchCV(logreg_reg_pipe, param_grid = logreg_reg_pipe_params, cv = 5, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_logreg_reg_pipe.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Metric (Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_cross_val_gs(gs_logreg_reg_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_accuracy_scores_gs(model = gs_logreg_reg_pipe, xtrain = X_train, xtest = X_test, ytrain = y_train, ytest = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this model remains slightly overfit to the training data, accuracy improved to ~ 76%. This still seems fairly low, so more models will be explored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_pipe = Pipeline([\n",
    "    ('tfidf', tfidf_transformer),\n",
    "    ('nb', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search Over Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_pipe_params = {\n",
    "    'tfidf__tfidf__stop_words': [nltk_stopwords],\n",
    "    'tfidf__tfidf__min_df': [3],\n",
    "    'tfidf__tfidf__max_df': [0.20],\n",
    "    'tfidf__tfidf__ngram_range': [(1,2)],\n",
    "    'tfidf__tfidf__max_features': [None]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_nb_pipe = GridSearchCV(nb_pipe, param_grid = nb_pipe_params, cv = 5, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_nb_pipe.fit(X_bayes_train, y_bayes_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Metric (Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_cross_val_gs(gs_nb_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_accuracy_scores_gs(gs_nb_pipe, X_bayes_train, y_bayes_train, X_bayes_test, y_bayes_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model also remains overfit to the training data. However, if hyperparameters, such as `min_df`, were adjusted to reduce variance, it also resulted in lower cross_val and testing scores. Therefore, the hyperparameters will be kept as is. The testing accuracy of 79.1% is an imporvement over the previous two models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pipe_robust = Pipeline([\n",
    "    ('tfidf', tfidf_transformer),\n",
    "    ('rs', RobustScaler(with_centering = False)),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# In this case, robust scaler performed better than standard scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search Over Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pipe_params = {\n",
    "    'tfidf__tfidf__stop_words':['english'],\n",
    "    'tfidf__tfidf__min_df': [8],\n",
    "    'tfidf__tfidf__max_df': [0.45],\n",
    "    'tfidf__tfidf__ngram_range': [(1,1)],\n",
    "    'knn__n_neighbors' : [10],\n",
    "    'knn__metric':['euclidean'],\n",
    "    'knn__weights': ['distance']    \n",
    "}\n",
    "# Only best params remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_knn_pipe_robust = GridSearchCV(knn_pipe_robust, knn_pipe_params, cv = 5, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_knn_pipe_robust.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Metric (Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_cross_val_gs(gs_knn_pipe_robust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_accuracy_scores_gs(gs_knn_pipe_robust, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KNN is one of my worst-performing models. Hyperparameters were adjusted to reduce `max_df` to 0.2, and `neighbors` were reduced to 3, but the training accuracy remained 100%. The high dimensionality of this data and the outliers in some of the numerical features likely do not lend themselves well to this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_pipe = Pipeline([\n",
    "    ('tfidf', tfidf_transformer),\n",
    "    ('dt', DecisionTreeClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search Over Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_pipe_params = {\n",
    "    'tfidf__tfidf__stop_words': ['english'],\n",
    "    'tfidf__tfidf__min_df': [2],\n",
    "    'tfidf__tfidf__max_df': [0.6],\n",
    "    'tfidf__tfidf__ngram_range': [(1,2)],\n",
    "    'dt__min_samples_split': [7],\n",
    "    'dt__min_samples_leaf': [1],\n",
    "    'dt__max_depth': [15]\n",
    "}\n",
    "# Only best params to limit overfitting without lowering testing and cross val scores shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_dt_pipe = GridSearchCV(dt_pipe, param_grid = dt_pipe_params, cv = 5, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_dt_pipe.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_cross_val_gs(gs_dt_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_accuracy_scores_gs(gs_dt_pipe, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree was also a weak model for this dataset. It was very difficult to balance overfitting without lowering the cross-validation and testing scores. This model ended up performing with 69.1% accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6: Bagging Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_pipe = Pipeline([\n",
    "    ('tfidf', tfidf_transformer),\n",
    "    ('bc', BaggingClassifier(random_state = RANDOM_STATE))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search Over Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_pipe_params = {\n",
    "    'tfidf__tfidf__stop_words':['english'],\n",
    "    'tfidf__tfidf__min_df': [2],\n",
    "    'tfidf__tfidf__max_df': [0.3],\n",
    "    'tfidf__tfidf__ngram_range': [(1,2)],\n",
    "    'bc__n_estimators':[100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_bag_pipe = GridSearchCV(bag_pipe, param_grid = bag_pipe_params, cv = 5, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_bag_pipe.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Metric (Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_cross_val_gs(gs_bag_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_accuracy_scores_gs(gs_bag_pipe, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bagging classifier is able to predict posts with 75.2% accuracy, but it is extremely overfit to the training data. Tuning of hyperparameters did not seem to help. In order to reduce overfitting, this might require manual pruning of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 7: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipe = Pipeline([\n",
    "    ('tfidf', tfidf_transformer),\n",
    "    ('forest', RandomForestClassifier(random_state = 1234))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search Over Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipe_params = {\n",
    "    'tfidf__tfidf__stop_words' : ['english'],\n",
    "    'tfidf__tfidf__min_df': [12],\n",
    "    'tfidf__tfidf__max_df': [0.4],\n",
    "    'tfidf__tfidf__ngram_range': [(1,2)],\n",
    "    'forest__criterion': ['entropy'],\n",
    "    'forest__max_depth': [12],\n",
    "    'forest__ccp_alpha' : [0.001]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_rf_pipe = GridSearchCV(rf_pipe, param_grid = rf_pipe_params, cv = 5, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_rf_pipe.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_cross_val_gs(gs_rf_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_accuracy_scores_gs(gs_rf_pipe, X_test, y_test, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest appears to be a stronger model for this data set. Both the cross_val score and testing scores were slightly higher than the training score and likely relates to the randommness of the features that were selected for each tree. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 8: Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting = VotingClassifier(\n",
    "    estimators = [\n",
    "    ('logreg', gs_logreg_reg_pipe),\n",
    "    ('dt', gs_dt_pipe),\n",
    "    ('bagging', gs_bag_pipe),\n",
    "    ('forest', gs_rf_pipe)\n",
    "])\n",
    "# Multinomial Bayes not included in vote because of incompatible data (negative values for polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_accuracy_scores_gs(voting, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The voting classifier did not perform as well as I had hoped. It was able to predict posts with 76.5% accuracy. Likely, the models included in the voting classifer were too overfit, and this carried into this ensemble model. To be able to use this more effectively, I think that the vectorized text should be pruned for irrelevant features and the hyperparameters for each model should be re-tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, building a classification model to distinguish posts from Life Pro Tips from posts from Unethical Life Pro Tips was a difficult task. Due to the high dimensionality of the text data, several models had a tendency to overfit, and in many cases, reducing the variance of the data also resulted in a reduction in the test and cross-val scores. The accuracy scores for all 9 models are shown below. \n",
    "\n",
    "|Model|Train Accuracy|Test Accuracy|\n",
    "|---|---|---|\n",
    "|Baseline/Null|0.505|0.501|\n",
    "|Logistic Regression (No Regularization)|0.845|0.724|\n",
    "|Logistic Regression with L2 Regularization|0.838|0.760|\n",
    "|Multinomial Naive Bayes|0.902|0.790|\n",
    "|KNN (10 Neighbors)|1.0|0.658|\n",
    "|Decision Tree|0.780|0.691|\n",
    "|Bagging Classifier|1.0|0.752|\n",
    "|Random Forest|0.699|0.780|\n",
    "|Voting Classifier|0.949|0.765|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By just examining the test accuracy, the Baseline model, KNN, and Decision Tree, an Bagging Classifier can immediately be eliminated. However, the other posts also performed in a similar range. Although the ideal model would correctly identify all posts, if all models are performing with similar accuracy, I would also like to look at the sensitivity (true positive rate) for each model. For this particular problem, it should be a priority to minimize the number of Unethical Life Pro Tips that are being classified as normal, ethical Life Pro Tips, and thus, minimizing False Negatives should be a goal. Therefore, the sensitivity will be calculated for Logistic Regression with L2 Regularization, Multinomial Naive Bayes, and the Voting Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sensitivity Calculations (Results described below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Logistic Regression with L2:')\n",
    "get_training_sensitivity(y_train, gs_logreg_reg_pipe.predict(X_train))\n",
    "get_testing_sensitivity(y_test, gs_logreg_reg_pipe.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Multinomial Naive Bayes:')\n",
    "get_training_sensitivity(y_bayes_train, gs_nb_pipe.predict(X_bayes_train))\n",
    "get_testing_sensitivity(y_bayes_test, gs_nb_pipe.predict(X_bayes_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Random Forest:')\n",
    "get_training_sensitivity(y_train, gs_rf_pipe.predict(X_train))\n",
    "get_training_sensitivity(y_test, gs_rf_pipe.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Voting Classifier:')\n",
    "get_training_sensitivity(y_train, voting.predict(X_train))\n",
    "get_testing_sensitivity(y_test, voting.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion of Sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Model|Train Accuracy|Test Accuracy|Test Sensitivity|\n",
    "|---|---|---|---|\n",
    "|Logistic Regression with L2 Regularization|0.838|0.760|0.863|\n",
    "|Multinomial Naive Bayes|0.902|0.791|0.885|\n",
    "|Random Forest|0.699|0.780|0.906|\n",
    "|Voting Classifier|0.949|0.765|0.745|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the table above, although accuracy scores for each of these four models were not really high, the Logistic Regression, Multinomial Naive bayes, and the Random Forest did a pretty good job of minimizing the number of Unethical Life Pro Tips that were classifed as ordinary Life Pro Tips. Because it is likely more dangerous for the Unethical Life Pro Tips to be posted to a Life Pro Tips forum, this is good. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the Random Forest can identify all posts with 78% accuracy and has a true positive rate of 90.6%, I will choose this model for further evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand the performance of the Random Forest Classifier, I will begin by plotting the confusin matrix for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(gs_rf_pipe, X_test, y_test, normalize = 'true', cmap = custom_cmap);\n",
    "plt.title('Confusion Matrix for Random Forest Classifier', fontdict = {'fontsize':14}, pad = 10)\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above confusion matrix shows (*clockwise, from upper left corner*) the true negatives, false positives, false negatives, and true positives for the testing predictions generated by the Random Forest Classifier. The data is normalized by row. This plot shows that this model was very good at correctly classifying the Unethical Life Pro Tips posts; it did it with 91% accuracy. However, the model performed around baseline for Life Pro Tips posts, with only 55% accuracy. Although the perfect model would correctly predict both classes equally as well, as previously mentioned, because the Unethical Life Pro Tips subreddit gives advice that is unethical, if accurately classifying one of the classes had to be priortized, I would choose to correctly classify the unethical posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(gs_rf_pipe, X_test, y_test);\n",
    "plt.title('ROC Curve for Random Forest Classifier (AUC = 0.82)', fontdict = {'fontsize':15}, pad = 12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC Curve shows the performance of the Random Forest Classifier at different thresholds, and the AUC tell us how well our model is able to distinguish between the two classes (Unethical Life Pro Tips and Life Pro Tips) at the various thresholds. A perfect score is 1, and the worst score is 0.5. This model has an AUC of 0.82, so it performs intermediately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the best model was the Random Forest, we are able to view the feature importances for the model. In total, this model had 1374 features (1369 words + 5 numeric). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Word Features\n",
    "tfidf_feature_names = gs_rf_pipe.best_estimator_.steps[0][1].transformers_[0][1].get_feature_names()\n",
    "\n",
    "# Combine with Numeric Features\n",
    "rf_feature_names = ['num_comments', 'score', 'post_length_char', 'post_length_words', 'polarity'] + tfidf_feature_names\n",
    "\n",
    "# Get Feature Importances\n",
    "rf_feature_importances = gs_rf_pipe.best_estimator_.steps[1][1].feature_importances_\n",
    "\n",
    "# Plot Data Frame\n",
    "feature_importances_df = pd.DataFrame({'Feature':rf_feature_names, 'Importance':rf_feature_importances}).sort_values('Importance', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = [medium_blue, medium_blue, light_blue, light_blue, light_blue, light_blue, medium_blue, light_blue, medium_blue, medium_blue, medium_blue, light_blue, light_blue, light_blue, medium_blue]\n",
    "plt.figure(figsize = (16, 8))\n",
    "sns.barplot(x = feature_importances_df['Importance'][:15], y = feature_importances_df['Feature'].str.title()[:15], orient='h', palette = palette, edgecolor = 'black');\n",
    "plt.title('Top 15 Features for Random Forest Classifier', fontdict = {'fontsize':20}, pad = 15)\n",
    "plt.xlabel('Importance', fontdict = {'fontsize':15}, labelpad = 7)\n",
    "plt.ylabel('Feature', fontdict = {'fontsize':15}, labelpad = 7)\n",
    "plt.xticks(fontsize = 12)\n",
    "plt.yticks(fontsize = 12)\n",
    "plt.tight_layout();\n",
    "\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "LPT = mpatches.Patch(color=light_blue, label='Life Pro Tips')\n",
    "ULPT = mpatches.Patch(color=medium_blue, label='Unethical Life Pro Tips')\n",
    "\n",
    "plt.legend(handles=[LPT, ULPT], loc= 'lower right', fontsize = 14, title = 'Found Most Frequently In')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graph shows the top 15 features and their importances. It is important to note that, because the text was preprocessed using Porter stemming,the words appear as stems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the 15 words that were determined to have the highest feature importances, eight occurred more frequently in the Life Pro Tips corpus. Those words included: \"letter\", \"hear\", \"wall\", \"type\", \"batteri\", \"later\", \"eye\", \"youtub\". Many of these words could be related to practical tips, such as \"letter\", \"wall\", \"type\", \"batteri\", \"eye\", and \"youtub\". Seven of the words occurred more frequently in the Unethical Life Pro Tips corpus (\"young\", \"forev\", \"fault\", \"youtube\", \"zero\", \"walmart\", \"red\"). Out of context, these words do not appear to be overly unethical. One important thing that I notice is that \"youtub\" and \"youtube\" are likely mispellings of the same word. To improve the model and reduce dimensionality, spelling errors could be fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis on More Frequent Words by Subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide Words by Where They Appear More\n",
    "\n",
    "more_lpt = []\n",
    "more_ulpt = []\n",
    "equal = []\n",
    "\n",
    "lpt_df = stemmed_labeled_words_df[stemmed_labeled_words_df['is_unethical'] == 0]\n",
    "ulpt_df = stemmed_labeled_words_df[stemmed_labeled_words_df['is_unethical'] == 1]\n",
    "\n",
    "for word in tfidf_feature_names:\n",
    "    if lpt_df[word].sum() > ulpt_df[word].sum():\n",
    "        more_lpt.append(word)\n",
    "    elif ulpt_df[word].sum() > lpt_df[word].sum():\n",
    "        more_ulpt.append(word)\n",
    "    else:\n",
    "        equal.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpt_rf_sentiment = TextBlob(' '.join(more_lpt))\n",
    "lpt_rf_sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ulpt_rf_sentiment = TextBlob(' '.join(more_ulpt))\n",
    "ulpt_rf_sentiment.polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the sentiment analysis, it appears that most words in both subreddits were judged to be neutral, especially if it refered to a noun. As a whole, the words found more frequently in the Life Pro Tips category were slighlty less neutral (more positive) than the words found more frequently in the Unethical Life Pro Tips Subreddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, I was tasked with building a classification model that could accurately distinguish posts from the r/LifeProTipsSubreddit from the r/UnethicalLifeProTips Subreddit. The best-performing model was a Random Forest Classifier with an accuracy of 78% and a sensitivity of 90.6%. This model is good at accurately predicting posts from the r/UnethicalLifeProTips Subreddit, but it performs almost at Baseline for posts from r/LifeProTips. Although the consequences of posting genuine Life Pro Tips to r/UnethicalLifeProTips are probably minimal (unless it encourages people to behave more ethically!), to truly accomplish the task at hand, prior to being production-ready, we should try to improve the overall accuracy score. \n",
    "\n",
    "To begin the next round of improvements, we will learn from our models and manually prune features from the vector of words. Once the dimensionality of the models has been reduced, we will be able to tune our models in hopes of improving the overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
